import_data_to_db(con)
# Process and analyze data
process_and_analyze_data(con)
log_info("Data processing pipeline completed successfully")
}, error = function(e) {
log_error(sprintf("Pipeline failed: %s", conditionMessage(e)))
})
}
#' Import data into the database
#'
#' @param con Database connection object
#' @return NULL
import_data_to_db <- function(con) {
ts_rows <- run_pipeline_step("Importing TimeSeries data", import_data,
file.path("..", "data", "processed", "combined_time_series.csv"), con, "TimeSeries")
gd_rows <- run_pipeline_step("Importing GriddedData", import_data,
file.path("..", "data", "processed", "gridded_data.csv"), con, "GriddedData")
log_info(sprintf("Imported rows - TimeSeries: %d, GriddedData: %d", ts_rows, gd_rows))
if ((is.null(ts_rows) || ts_rows == 0) && (is.null(gd_rows) || gd_rows == 0)) {
stop("No data imported into any table. Stopping pipeline.")
}
}
#' Process and analyze data
#'
#' @param con Database connection object
#' @return NULL
process_and_analyze_data <- function(con) {
sql_dir <- file.path("..", "sql")
run_pipeline_step("Processing data", execute_sql_file, con, file.path(sql_dir, "process_data.sql"))
run_pipeline_step("Running diagnostics", execute_sql_file, con, file.path(sql_dir, "run_diagnostics.sql"))
exploration_results <- run_pipeline_step("Data exploration", execute_sql_file, con, file.path(sql_dir, "explore_data.sql"))
if (!is.null(exploration_results)) {
log_info("Data Exploration Results:")
write_exploration_results(exploration_results)
} else {
log_warn("Data exploration failed to produce results.")
}
}
#' Write exploration results to a markdown file
#'
#' @param results List of data frames containing exploration results
#' @return NULL
write_exploration_results <- function(results) {
output_file <- file.path("..", "docs", "analysis_results.md")
con <- file(output_file, "w")
on.exit(close(con), add = TRUE)
writeLines("# Global Temperature Analysis Results\n", con)
for (i in seq_along(results)) {
writeLines(sprintf("\n## Result Set %d\n", i), con)
writeLines("```", con)
capture.output(print(results[[i]]), file = con)
writeLines("```\n", con)
}
log_info(sprintf("Analysis results written to %s", output_file))
}
# Run the main function
main()
# runner.R
# Load required libraries and source utility functions
library(logger)
source("utils.R")
#' Main function to run the data processing pipeline
#'
#' @return NULL
#' @export
main <- function() {
log_info("Starting data processing pipeline")
tryCatch({
# Initialize database connection
con <- db_connect(verbose = as.logical(Sys.getenv("VERBOSE", "FALSE")))
on.exit(dbDisconnect(con), add = TRUE)
# Run pipeline steps
run_pipeline_step("Downloading data", download_data)
run_pipeline_step("Converting data", convert_data)
run_pipeline_step("Setting up database", setup_database, con)
# Import data
import_data_to_db(con)
# Process and analyze data
process_and_analyze_data(con)
log_info("Data processing pipeline completed successfully")
}, error = function(e) {
log_error(sprintf("Pipeline failed: %s", conditionMessage(e)))
})
}
#' Import data into the database
#'
#' @param con Database connection object
#' @return NULL
import_data_to_db <- function(con) {
ts_rows <- run_pipeline_step("Importing TimeSeries data", import_data,
file.path("..", "data", "processed", "combined_time_series.csv"), con, "TimeSeries")
gd_rows <- run_pipeline_step("Importing GriddedData", import_data,
file.path("..", "data", "processed", "gridded_data.csv"), con, "GriddedData")
log_info(sprintf("Imported rows - TimeSeries: %d, GriddedData: %d", ts_rows, gd_rows))
if ((is.null(ts_rows) || ts_rows == 0) && (is.null(gd_rows) || gd_rows == 0)) {
stop("No data imported into any table. Stopping pipeline.")
}
}
#' Process and analyze data
#'
#' @param con Database connection object
#' @return NULL
process_and_analyze_data <- function(con) {
sql_dir <- file.path("..", "sql")
run_pipeline_step("Processing data", execute_sql_file, con, file.path(sql_dir, "process_data.sql"))
run_pipeline_step("Running diagnostics", execute_sql_file, con, file.path(sql_dir, "run_diagnostics.sql"))
exploration_results <- run_pipeline_step("Data exploration", execute_sql_file, con, file.path(sql_dir, "explore_data.sql"))
if (!is.null(exploration_results)) {
log_info("Data Exploration Results:")
write_exploration_results(exploration_results)
} else {
log_warn("Data exploration failed to produce results.")
}
}
#' Write exploration results to a markdown file
#'
#' @param results List of data frames containing exploration results
#' @return NULL
write_exploration_results <- function(results) {
output_file <- file.path("..", "docs", "analysis_results.md")
con <- file(output_file, "w")
on.exit(close(con), add = TRUE)
writeLines("# Global Temperature Analysis Results\n", con)
for (i in seq_along(results)) {
writeLines(sprintf("\n## Result Set %d\n", i), con)
writeLines("```", con)
capture.output(print(results[[i]]), file = con)
writeLines("```\n", con)
}
log_info(sprintf("Analysis results written to %s", output_file))
}
# Run the main function
main()
# runner.R
# Load required libraries and source utility functions
library(logger)
source("utils.R")
#' Main function to run the data processing pipeline
#'
#' @return NULL
#' @export
main <- function() {
log_info("Starting data processing pipeline")
tryCatch({
# Initialize database connection
con <- db_connect(verbose = as.logical(Sys.getenv("VERBOSE", "FALSE")))
on.exit(dbDisconnect(con), add = TRUE)
# Run pipeline steps
run_pipeline_step("Downloading data", download_data)
run_pipeline_step("Converting data", convert_data)
run_pipeline_step("Setting up database", setup_database, con)
# Import data
import_data_to_db(con)
# Process and analyze data
process_and_analyze_data(con)
log_info("Data processing pipeline completed successfully")
}, error = function(e) {
log_error(sprintf("Pipeline failed: %s", conditionMessage(e)))
})
}
#' Import data into the database
#'
#' @param con Database connection object
#' @return NULL
import_data_to_db <- function(con) {
ts_rows <- run_pipeline_step("Importing TimeSeries data", import_data,
file.path("..", "data", "processed", "combined_time_series.csv"), con, "TimeSeries")
gd_rows <- run_pipeline_step("Importing GriddedData", import_data,
file.path("..", "data", "processed", "gridded_data.csv"), con, "GriddedData")
log_info(sprintf("Imported rows - TimeSeries: %d, GriddedData: %d", ts_rows, gd_rows))
if ((is.null(ts_rows) || ts_rows == 0) && (is.null(gd_rows) || gd_rows == 0)) {
stop("No data imported into any table. Stopping pipeline.")
}
}
#' Process and analyze data
#'
#' @param con Database connection object
#' @return NULL
process_and_analyze_data <- function(con) {
sql_dir <- file.path("..", "sql")
run_pipeline_step("Processing data", execute_sql_file, con, file.path(sql_dir, "process_data.sql"))
run_pipeline_step("Running diagnostics", execute_sql_file, con, file.path(sql_dir, "run_diagnostics.sql"))
exploration_results <- run_pipeline_step("Data exploration", execute_sql_file, con, file.path(sql_dir, "explore_data.sql"))
if (!is.null(exploration_results)) {
log_info("Data Exploration Results:")
write_exploration_results(exploration_results)
} else {
log_warn("Data exploration failed to produce results.")
}
}
#' Write exploration results to a markdown file
#'
#' @param results List of data frames containing exploration results
#' @return NULL
write_exploration_results <- function(results) {
output_file <- file.path("..", "docs", "analysis_results.md")
con <- file(output_file, "w")
on.exit(close(con), add = TRUE)
writeLines("# Global Temperature Analysis Results\n", con)
for (i in seq_along(results)) {
writeLines(sprintf("\n## Result Set %d\n", i), con)
writeLines("```", con)
capture.output(print(results[[i]]), file = con)
writeLines("```\n", con)
}
log_info(sprintf("Analysis results written to %s", output_file))
}
# Run the main function
main()
# runner.R
# Set working directory to the script's location
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
# Source utility functions and other necessary scripts
source("utils.R")
# Load required packages and set default environment variables
load_required_packages()
set_default_env_variables()
# utils.R
library(DBI)
library(dplyr)
library(httr)
library(ncdf4)
library(odbc)
library(readr)
library(curl)
convert_data <- function() {
log_message("Starting data conversion...")
raw_dir <- file.path("..", "data", "raw")
processed_dir <- file.path("..", "data", "processed")
dir.create(processed_dir, showWarnings = FALSE, recursive = TRUE)
timeseries_csv <- file.path(processed_dir, "combined_time_series.csv")
gridded_csv <- file.path(processed_dir, "gridded_data.csv")
# Check if CSVs already exist
if (file.exists(timeseries_csv) && file.exists(gridded_csv)) {
log_message("CSV files already exist. Skipping conversion.")
return(TRUE)
}
# Process ASC file
asc_file <- file.path(raw_dir, "aravg.ann.land_ocean.90S.90N.v6.0.0.202407.asc")
if (file.exists(asc_file) && !file.exists(timeseries_csv)) {
tryCatch({
ts_data <- read.table(asc_file, header = FALSE, fill = TRUE, stringsAsFactors = FALSE)
if (ncol(ts_data) >= 2) {
ts_data <- ts_data[, 1:2]
colnames(ts_data) <- c("Year", "Temperature")
write.csv(ts_data, timeseries_csv, row.names = FALSE)
log_message("Timeseries data converted successfully.")
} else {
log_message("ASC file does not have expected number of columns.", "ERROR")
}
}, error = function(e) {
log_message(sprintf("Error converting ASC file: %s", conditionMessage(e)), "ERROR")
})
} else {
log_message("ASC file not found or CSV already exists. Skipping timeseries conversion.")
}
# Process NC file
nc_file <- file.path(raw_dir, "NOAAGlobalTemp_v6.0.0_gridded_s185001_e202407_c20240806T153047.nc")
if (file.exists(nc_file) && file.size(nc_file) > 0 && !file.exists(gridded_csv)) {
tryCatch({
nc <- nc_open(nc_file)
var_name <- names(nc$var)[1]
df <- expand.grid(Longitude = ncvar_get(nc, "lon"), Latitude = ncvar_get(nc, "lat"), Time = ncvar_get(nc, "time"))
df$Temperature <- as.vector(ncvar_get(nc, var_name))
nc_close(nc)
write.csv(df, gridded_csv, row.names = FALSE)
log_message("Gridded data converted successfully.")
}, error = function(e) {
log_message(sprintf("Error converting NC file: %s", conditionMessage(e)), "ERROR")
})
} else {
log_message("NC file not found, empty, or CSV already exists. Skipping gridded data conversion.")
}
log_message("Data conversion completed.")
TRUE
}
db_connect <- function(verbose = FALSE) {
tryCatch({
con <- dbConnect(odbc::odbc(),
Driver = "ODBC Driver 17 for SQL Server",
Server = Sys.getenv("SQL_SERVER_NAME"),
Database = Sys.getenv("SQL_DATABASE_NAME"),
Trusted_Connection = "Yes")
if(verbose) log_message("Connected to database successfully.")
con
}, error = function(e) {
log_message(sprintf("Failed to connect to database: %s", conditionMessage(e)), "ERROR")
stop(e)
})
}
debug_file_info <- function(file_path) {
if (file.exists(file_path)) {
info <- file.info(file_path)
log_message(sprintf("File: %s, Size: %d bytes, Modified: %s",
file_path, info$size, info$mtime))
if (grepl("\\.csv$", file_path)) {
tryCatch({
data <- read.csv(file_path, nrows = 5)
log_message(sprintf("CSV Preview (first 5 rows):\n%s",
paste(capture.output(print(data)), collapse = "\n")))
}, error = function(e) {
log_message(sprintf("Error reading CSV: %s", conditionMessage(e)), "ERROR")
})
}
} else {
log_message(sprintf("File not found: %s", file_path), "ERROR")
}
}
debug_env_vars <- function() {
env_vars <- c("SQL_SERVER_NAME", "SQL_DATABASE_NAME", "VERBOSE")
for (var in env_vars) {
log_message(sprintf("%s: %s", var, Sys.getenv(var)))
}
}
debug_db_connection <- function(con) {
tryCatch({
tables <- dbListTables(con)
log_message(sprintf("Connected to database. Tables: %s",
paste(tables, collapse = ", ")))
}, error = function(e) {
log_message(sprintf("Database connection error: %s", conditionMessage(e)), "ERROR")
})
}
debug_download <- function(url, dest_file) {
tryCatch({
download.file(url, dest_file, mode = "wb")
if (file.exists(dest_file) && file.size(dest_file) > 0) {
log_message(sprintf("Successfully downloaded: %s (Size: %d bytes)",
dest_file, file.size(dest_file)))
} else {
log_message(sprintf("Download failed or file is empty: %s", dest_file), "ERROR")
}
}, error = function(e) {
log_message(sprintf("Error downloading %s: %s", url, conditionMessage(e)), "ERROR")
})
}
download_data <- function() {
base_url <- "https://www.ncei.noaa.gov/data/noaa-global-surface-temperature/v6/access/"
timeseries_url <- paste0(base_url, "timeseries/")
gridded_url <- paste0(base_url, "gridded/")
download_dir <- file.path("..", "data", "raw")
dir.create(download_dir, showWarnings = FALSE, recursive = TRUE)
csv_dir <- file.path("..", "data", "processed")
dir.create(csv_dir, showWarnings = FALSE, recursive = TRUE)
timeseries_csv <- file.path(csv_dir, "combined_time_series.csv")
gridded_csv <- file.path(csv_dir, "gridded_data.csv")
# Check if CSVs already exist
if (file.exists(timeseries_csv) && file.exists(gridded_csv)) {
return(TRUE)
}
download_file <- function(url, destfile) {
if (file.exists(destfile)) {
log_message(sprintf("File %s already exists. Skipping download.", destfile))
return(TRUE)
}
tryCatch({
curl_download(url, destfile, mode = "wb")
TRUE
}, error = function(e) {
log_message(sprintf("Error downloading %s: %s", url, conditionMessage(e)), "ERROR")
FALSE
})
}
# Get list of all ASC files
asc_files <- httr::GET(timeseries_url) %>%
httr::content("text") %>%
xml2::read_html() %>%
xml2::xml_find_all("//a[contains(@href, '.asc')]") %>%
xml2::xml_attr("href")
asc_success <- sapply(asc_files, function(file) {
download_file(paste0(timeseries_url, file), file.path(download_dir, file))
})
# Download NC file
nc_file <- "NOAAGlobalTemp_v6.0.0_gridded_s185001_e202407_c20240806T153047.nc"
nc_success <- download_file(paste0(gridded_url, nc_file), file.path(download_dir, nc_file))
log_message(sprintf("Downloaded %d/%d ASC files and %s NC file", sum(asc_success), length(asc_success), if(nc_success) "1/1" else "0/1"))
all(asc_success) && nc_success
}
execute_sql_file <- function(con, filename) {
if (file.exists(filename)) {
tryCatch({
sql_content <- paste(readLines(filename), collapse = "\n")
sql_statements <- strsplit(sql_content, ";")[[1]]
results <- list()
for (statement in sql_statements) {
if (trimws(statement) != "") {
result <- dbGetQuery(con, statement)
if (!is.null(result) && nrow(result) > 0) {
results[[length(results) + 1]] <- result
}
}
}
log_message(sprintf("Executed SQL file: %s", filename))
return(results)
}, error = function(e) {
log_message(sprintf("Error executing %s: %s", filename, conditionMessage(e)), "ERROR")
return(NULL)
})
} else {
log_message(sprintf("%s file not found.", filename), "ERROR")
return(NULL)
}
}
import_data <- function(csv_path, con, table_name) {
if (file.exists(csv_path)) {
data <- read.csv(csv_path)
# Delete existing data
dbExecute(con, sprintf("DELETE FROM %s", table_name))
# Import new data
dbWriteTable(con, table_name, data, append = TRUE, row.names = FALSE)
return(nrow(data))
} else {
log_message(sprintf("File not found: %s", csv_path), "ERROR")
return(0)
}
}
load_required_packages <- function() {
required_packages <- c("curl", "DBI", "dplyr", "httr", "ncdf4", "odbc", "readr")
sapply(required_packages, function(package) {
if(!require(package, character.only = TRUE)) {
install.packages(package, dependencies = TRUE)
library(package, character.only = TRUE)
}
})
}
log_message <- function(message, level = "INFO") {
log_entry <- sprintf("[%s] %s: %s\n", format(Sys.time(), "%Y-%m-%d %H:%M:%S"), level, message)
cat(log_entry)
write(log_entry, file = "pipeline.log", append = TRUE)
}
run_pipeline_step <- function(step_name, fun, ...) {
log_message(sprintf("Starting %s", step_name))
result <- tryCatch(fun(...), error = function(e) {
log_message(sprintf("Error in %s: %s", step_name, conditionMessage(e)), "ERROR")
NULL
})
log_message(sprintf("%s %s.", step_name, if(is.null(result)) "failed" else "completed successfully"))
result
}
set_default_env_variables <- function() {
if(Sys.getenv("SQL_SERVER_NAME") == "") Sys.setenv(SQL_SERVER_NAME = "(local)")
if(Sys.getenv("SQL_DATABASE_NAME") == "") Sys.setenv(SQL_DATABASE_NAME = "GlobalTemperatureAnalysis")
if(Sys.getenv("VERBOSE") == "") Sys.setenv(VERBOSE = "FALSE")
}
setup_database <- function(con) {
sql_file <- file.path("..", "sql", "setup_database.sql")
if (file.exists(sql_file)) {
sql_script <- gsub("\\$\\(SQL_DATABASE_NAME\\)", Sys.getenv("SQL_DATABASE_NAME"),
paste(readLines(sql_file), collapse = "\n"))
tryCatch({
dbExecute(con, sql_script)
log_message("Database setup completed successfully.")
TRUE
}, error = function(e) {
log_message(sprintf("Error executing SQL script: %s", conditionMessage(e)), "ERROR")
FALSE
})
} else {
log_message(sprintf("SQL file not found: %s", sql_file), "ERROR")
FALSE
}
}
# runner.R
# Set working directory to the script's location
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
# Source utility functions and other necessary scripts
source("utils.R")
# Load required packages and set default environment variables
load_required_packages()
set_default_env_variables()
main <- function() {
log_message("Starting data processing pipeline", "INFO")
tryCatch({
con <- db_connect(verbose = FALSE)
on.exit(dbDisconnect(con))
if (!run_pipeline_step("Downloading data", download_data)) {
stop("Data download failed. Stopping pipeline.")
}
if (!run_pipeline_step("Converting data", convert_data)) {
stop("Data conversion failed. Stopping pipeline.")
}
if (!run_pipeline_step("Setting up database", setup_database, con)) {
stop("Database setup failed. Stopping pipeline.")
}
ts_rows <- run_pipeline_step("Importing TimeSeries data", import_data,
file.path("..", "data", "raw", "combined_time_series.csv"), con, "TimeSeries")
gd_rows <- run_pipeline_step("Importing GriddedData", import_data,
file.path("..", "data", "raw", "gridded_data.csv"), con, "GriddedData")
log_message(sprintf("Imported rows - TimeSeries: %d, GriddedData: %d", ts_rows, gd_rows))
if ((is.null(ts_rows) || ts_rows == 0) && (is.null(gd_rows) || gd_rows == 0)) {
stop("No data imported into any table. Stopping pipeline.")
}
sql_dir <- file.path("..", "sql")
run_pipeline_step("Processing data", execute_sql_file, con, file.path(sql_dir, "process_data.sql"))
run_pipeline_step("Running diagnostics", execute_sql_file, con, file.path(sql_dir, "run_diagnostics.sql"))
exploration_results <- run_pipeline_step("Data exploration", execute_sql_file, con, file.path(sql_dir, "explore_data.sql"))
if (!is.null(exploration_results)) {
log_message("Data Exploration Results:")
for (i in seq_along(exploration_results)) {
log_message(sprintf("Result set %d:", i))
print(exploration_results[[i]])
cat("\n")
}
} else {
log_message("Data exploration failed to produce results.", "WARNING")
}
log_message("Data processing pipeline completed successfully.", "INFO")
}, error = function(e) {
log_message(sprintf("Pipeline failed: %s", conditionMessage(e)), "ERROR")
})
}
main()
